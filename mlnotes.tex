\documentclass{book}[a5paper]
\usepackage[a5paper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{pgfplots, pgfplotstable}
\usepackage{float}

% Speed up the compile time for TIKZ
%\usepgfplotslibrary{external}
%\tikzexternalize

\newcommand{\placeholder}{[Insert Section Here]}

\title{Machine Learning}
\author{Zachary Ross}
\date{November 2019}

\begin{document}

\maketitle

% TODO: Give hypothesis functions their own section

% TODO: Give gradient descent its own section, probably going to reference
% hypothesis functions

\chapter{Linear Regression}

The first lesson covered by any elementary machine learning course goes as
follows: In the housing market, there are several predictors that go into how
much a house should cost. These are things such as plot area, number of
bedrooms, number of foreclosures, etc, and each of these have some kind of
contribution to the overall price. Given examples, we can see that a bigger plot
area or more bedrooms may increase the evaluation of a given house, while the
number of foreclosures may have the adverse effect. So using these values, can
we predict a given house's worth? 

\section{Uni-variate Linear Regression}

% TODO: Add more background to the subject, especially how this should only be
% used with discrete models

To start off, we examine the simpler case where only one of these inputs has any
effect on the price. Table~\ref{table:house_price} sets up a correspondence
between two variables of data, those being the lot area and sales price for a
list of houses. Given these sets of data, we might wish to find a relation
between the two. That is, we would like some function that might \emph{predict}
how much a house could sell for given it's lot area. 

\begin{table}
    \label{table:house_price}
    \centering
    \begin{tabular}{c|c}
        Lot Area (${ft}^2$) & Housing Prices \\
        \hline
        8450 & \$208,500 \\
        9600 & \$181,500 \\
        11250 & \$223,500 \\
        9550 & \$140,000 \\
        14260 & \$250,000 \\
        14115 & \$143,000 \\
        10084 & \$307,000 \\
        10382 & \$200,000 \\
        6120 & \$129,900 \\
        7420 & \$118,000 \\
        11200 & \$129,500 \\
        11924 & \$345,000 \\
        12968 & \$144,000 \\
        10652 & \$279,500 \\
        10920 & \$157,000 \\
        6120 & \$132,000 \\
        11241 & \$149,000 \\
        10791 & \$90,000 \\
        13695 & \$159,000 \\
        7560 & \$139,000 \\
        14215 & \$325,300 \\
        7449 & \$139,400 \\
        9742 & \$230,000 \\
        4224 & \$129,900 \\
        8246 & \$154,000 \\
        14230 & \$256,300 \\
        7200 & \$134,800 \\
        11478 & \$306,000 \\
        16321 & \$207,500 \\
    \end{tabular}
\end{table}

% TODO: Add more here
The idea here is to create what is called an \emph{uni-variate linear
regression}, which is a function that is meant to establish a relationship
between a single \emph{explanatory} or \emph{independent} variable and a single
\emph{scalar response} or \emph{dependent} variable.  


Let $X$ be the \emph{feature vector}, which is the vector containing all
explanatory variables. Note that $X^{(i)}$ is the $i$th \emph{feature} of $X$.
Let $y$ be the vector of dependent variables with $y^{(i)}$ being the dependent
variable corresponding to $X^{(i)}$. In establishing a linear regression, the
goal is to create a function of the form $y=mx+b$, that is we wish to create a
linear function that can almost accurately predict a result $y$ given a feature
$x$. This function, called the \emph{hypothesis}, will be used to estimate the
function that most accurately predicts $y$ given $X$. The hypothesis is seen as 
\begin{equation}
    h_{\theta}(x) = \theta_0 + \theta_1x
\end{equation}
, where the coefficient vector $\theta$ is the vector of \emph{weights} that we
wish to use in estimating $y$.

The issue that we are trying to solve here is finding the values of ${\theta}$
that not only predict new values given the hypothesis, but also can accurately
estimate the original values of the features that created the hypothesis. With
that said, we can view this problem instead as a function approximating the
values of the weights, $\theta$. Let $J(\theta)$ be this function. So for this
function, we must determine the weights.

Let $m$ be the amount of features in $X$ and let $1 \leq i \leq m$. Consider
each point $(X^{(i)}, y^{(i)})$ on the $xy$-coordinate plane. Now we know that
$y^{(i)}$ is the real response for the feature $X^{(i)}$, while
$h_{\theta}(X^{(i)})$ is the theoretical response. In order to find the line
that best fits each point, we need to find the line that minimizes the distance
between each real and theoretical result, i.e.\ minimizing the distance between
$(X^{(i)}, y^{(i)})$ and $(X^{(i)}, h_{\theta}(X^{(i)}))$ for every $i$. 

This distance, called the \emph{variance}, can be measured for each individual
point by taking the squared difference between the theoretical value and the
real value, i.e.\  $(h_{\theta}(X^{(i)}) - y^{(i)})^2$. By summing them all we
get the \emph{residual sum of squares} which serves as a good estimate for the
overall variance that this line creates.
\begin{equation}
	J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(X^{(i)}) - y^{(i)})^2	
\end{equation}
The coefficient $\frac{1}{2m}$ is a summation factor that is discussed more in \placeholder.
 

Now that we have this function that determines the overall variance given a
weight vector $\theta$, we want to find the weights that minimize the overall
variance. Because this is a quadratic function, the shape taken by the equation
will be convex and positively increasing, so an absolute minimum is achievable.
Using calculus, we find this value when $\nabla J(\theta)=0$, however,
calculating this can often be an intensive or impossible feature on a computer.
Instead, we perform a gradient descent (discussed more in \placeholder) on
$\theta$ in $J(\theta)$, given by 

\begin{align*}
		&\theta_0 \leftarrow \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m(h_{\theta}(X^{(i)}) - y^{(i)})\\
		&\theta_1 \leftarrow \theta_1 - \alpha
        \frac{1}{m}\sum_{i=1}^m(h_{\theta}(X^{(i)}) - y^{(i)})X_1^{(i)}
\end{align*}

By prepending the value 1 to each row of the feature vector $X$, we extend the
length of each feature to contain 2 elements. This is precisely as many elements
per feature as there are weights. Doing this allows us to simplify the previous
expression by taking advantage of matrix properties. Because the residual sum of
squares, the step, and the summation factor are all real numbers, we can then
multiply each feature by these values during the descent. The gradient descent
iteration is then applied all at once.

\begin{equation}
    \theta \leftarrow \theta - \alpha
    \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(X^{(i)}) - y^{(i)})(X^{(i)})^T
\end{equation}
Note that for $(X^{(i)})^T$, the vector is transposed because we originally assumed
each feature to be a row vector while we assumed the weights to be a column
vector. So the transpose is a necessary conversion.

This step is then repeated until the values of the weights converge.  

\iffalse

That is, in reference to the housing case, we will use the plot
area to estimate cost. First, we need a couple of variables established:
\begin{equation}
    \begin{aligned}[c]
        X = 
        \begin{pmatrix}
            2,104 \text{ ft}^2 \\
            1,600 \text{ ft}^2 \\
            2,400 \text{ ft}^2 \\
            1,416 \text{ ft}^2 \\
            3,000 \text{ ft}^2 \\
            1,985 \text{ ft}^2 \\
            1,534 \text{ ft}^2 \\
            1,427 \text{ ft}^2 \\
            1,380 \text{ ft}^2 \\
            1,494 \text{ ft}^2
        \end{pmatrix}
    \end{aligned},
    \quad
    \begin{aligned}[c]
        y = 
        \begin{pmatrix}
            \$399,900 \\
            \$329,900 \\
            \$369,000 \\
            \$232,000 \\
            \$539,900 \\
            \$299,900 \\
            \$314,900 \\
            \$198,999 \\
            \$212,000 \\
            \$242,500
        \end{pmatrix}
    \end{aligned}
\end{equation}
That is, $X$ is the vector consisting of plot areas and $y$ is the vector
consisting of the corresponding prices.  Note that $X$ is called the
\emph{feature vector} and each row of this vector by association is called a
\emph{feature}.  This is constructed in a way that $X^{(i)}$, the $i$th feature
of $X$, should be the plot area corresponding to the cost $y^{(i)}$. 

To visualize, plotting the feature against it's cost value would look like the
scatterplot shown in Figure \ref{scatterplot}. 

\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                enlargelimits=false,
                xlabel={Plot Area (ft$^2$)},
                ylabel={House Cost (in \$100,000)},
                scaled y ticks=false,
                ytick={0, 100000,200000,300000,400000, 500000, 600000},
                yticklabels={0, 1, 2, 3, 4, 5, 6},
                xmin=0, xmax=3500,
                ymin=0, ymax=600000
            ]
            \addplot+[
                only marks,
                mark size=2.9pt]
            table
            {hdata.dat};
        \end{axis}
    \end{tikzpicture}
    \caption{Total house cost per square feet of area}
    \label{scatterplot}
\end{figure}

Looking at this graph, it's hopefully apparent that a trendline can be set to
the data that will, within a marginal error, give a cost for any associated plot
area. This trendline is associated with a general function, called the
\emph{hypothesis}, and notated as $h(x)$. 

\fi

\end{document}
